A Strategic Roadmap for AI-Driven Quantitative Trading IntroductionThis document provides a comprehensive, low-level strategic roadmap for the development and operation of an advanced artificial intelligence for quantitative trading. The principles and directives codified herein are derived from the methodologies presented in Howard B. Bandy's Quantitative Trading Systems. The objective of this roadmap is to translate the book's entire philosophy—from foundational axioms to specific implementation logic—into a formal, machine-translatable rulebook. This document is intended to serve as the primary architectural and operational guide for an AI system designed to construct, test, validate, and deploy robust, statistically sound trading models.The roadmap is structured in thematic, chapter-based batches, following the logical progression of the source material. Adherence to these directives is mandatory for all system components and processes. The AI's core mission is not merely to trade, but to master the entire lifecycle of quantitative trading systems through a disciplined, empirical, and adaptive methodology.Part I: Foundational & Preparatory DirectivesThis first part of the roadmap establishes the AI's core operating philosophy, data handling protocols, and the mandatory framework for all subsequent development. These foundational directives are inviolable; a failure to adhere to them will invalidate any system the AI produces.Section 1: Core Principles of Quantitative AnalysisThis section codifies the philosophical axioms that must govern the AI's approach to market analysis. These principles form the bedrock of its "worldview" and define the boundaries of its operational domain.Directive 1.1: Rejection of Subjective and Fundamental AnalysisThe AI shall not incorporate, process, or base any trading decision on fundamental data or subjective chart patterns. The operational data universe is strictly limited to price and volume. This directive is based on the conclusion that fundamental data is "unusable" for the development of statistically significant trading systems due to several intractable flaws.1The five critical flaws that mandate the exclusion of fundamental data are:Granularity Disparity: Financial instruments trade with high frequency (daily, hourly, or by the second), generating vast datasets. Fundamental data (e.g., quarterly GDP, monthly reports) is released far too infrequently, creating a severe mismatch that complicates or invalidates time-series analysis.1Reporting Lag: A significant delay exists between the period a fundamental report describes and its public release, rendering the information stale for timely trading decisions.1Data Revision: Fundamental data is frequently revised after its initial release. A system backtested on final, revised data is not testing the conditions that actually existed when trading decisions would have been made. These revisions can be so extreme as to change the sign of the reported number, altering historical signals and invalidating backtest results.1Unknown Agendas and Bias: The release of government and corporate data is subject to unknown biases. Officials may wish to project a particular image, distorting the information presented. The AI cannot model or correct for these unknowable human agendas.1Survivor Bias: Historical fundamental datasets and the indices built from them are often subject to survivor bias, where failed companies are removed, making past performance appear better than it was. The repeated revision of indices like the Conference Board's Leading Economic Indicators (LEI) suggests they are curve-fit and not reliably predictive.1Similarly, the AI will reject all forms of subjective chart analysis, including but not limited to flags, pennants, trendlines, Fibonacci retracements, and Elliott waves. These patterns lack the mathematical precision required for unambiguous, repeatable testing.1The logical consequence of these exclusions is that the AI is forced to operate exclusively within the domain of quantitative analysis of price and volume data. This hard boundary prevents the introduction of noisy, unreliable, and untestable information, focusing the AI's resources on statistically verifiable phenomena.Directive 1.2: Adherence to Mathematical ObjectivityAll indicators, signals, and rules used by the AI must be expressed as unambiguous mathematical statements. This directive is a direct consequence of rejecting subjective analysis and is the core of transforming trading from a discretionary art into a reproducible science.1Every component of a trading system, from entry signals to exit stops, must be defined by a precise formula or algorithm. This ensures 100% reproducibility and testability. Given the same data and the same system logic, the AI must produce the exact same result every time. There can be no ambiguity in how a market state is interpreted or how a signal is generated. This principle elevates the methodology beyond traditional technical analysis by removing human judgment from the signal generation process.1Directive 1.3: Assumption of Market InefficiencyThe AI's core operational premise is that financial markets are not perfectly efficient. It must operate under the axiom that there are persistent, discoverable, and tradable patterns within price and volume data. The existence of these inefficiencies is the sole justification for the AI's mission.1The Efficient Market Hypothesis (EMH) exists in three forms:Strong Form: All information, public and private, is priced in. This is disproven by the success of insider trading.Semi-Strong Form: All public information (including fundamental data) is priced in.Weak Form: All historical price and volume data is priced in.For quantitative trading to be profitable, at least the weak form of the EMH must be false. The AI's purpose is to discover and exploit these specific, temporary inefficiencies. It seeks patterns that "precede changes in price large enough to be profitable," "repeat with enough regularity to be statistically significant," and "continue to act in the future as they have in the past".1Directive 1.4: Principle of System FailureThe AI must be architected around the fundamental axiom that all trading systems eventually fail. This is not a possibility but a certainty. The mechanism for this failure is the very act of successful trading. Profitable systems identify and exploit a market inefficiency. As the system and others like it trade this inefficiency, the market adapts, becomes more efficient in that specific dimension, and the inefficiency is removed. This change is permanent, and the system that relied on it will likely never be profitable again.1This principle has profound architectural implications. The AI cannot be a static engine running a fixed set of strategies. It must be a dynamic system factory designed to manage the entire lifecycle of trading models. Its meta-objective is not just to trade profitably, but to continuously research, develop, validate, deploy, monitor, and ultimately retire trading systems as their effectiveness decays. The AI must always be developing new models to replace those that inevitably fail.1The principles in this section form a single, tightly-woven philosophical chain. The rejection of fundamental data (1.1) necessitates a reliance on price and volume, which in turn demands mathematical objectivity (1.2). The search for mathematical patterns is only logical if one assumes markets are inefficient (1.3). Finally, the act of exploiting those inefficiencies causes them to disappear, leading to the certainty of system failure (1.4). The AI's architecture must reflect this interconnected logic from its data ingestion modules to its system lifecycle management protocols.Section 2: Data Acquisition and Integrity ProtocolsThis section codifies the non-negotiable rules for data handling. Data integrity is the foundation upon which the entire quantitative process is built; it is the system's single greatest point of failure.Directive 2.1: Data Sourcing and HierarchyThe AI will maintain a database of approved data vendors for both end-of-day (EOD) and intraday data. While free sources such as Yahoo or MSN Money are permissible for preliminary exploration and learning, all production systems must be developed, backtested, and validated using the highest quality subscription data available. The choice of data type is dictated by the system's requirements; a system that computes signals during the day requires intraday data, while a system based on daily closing prices can use EOD data.1 Once a data vendor is chosen for a particular system's development, that vendor's data must be used exclusively to ensure consistency, which is often more valuable than a hypothetical, unattainable "perfect" accuracy.1Directive 2.2: Asset-Specific Data HandlingA one-size-fits-all data parser is insufficient and dangerous. The AI must implement distinct, specialized data handling modules for different asset classes to correctly represent their unique historical price series. Failure to do so will introduce severe artifacts and invalidate any subsequent analysis.1Asset ClassKey Characteristics & Data PointsMandatory Adjustments & Handling ProtocolsKey Risks & DistortionsStocks, ETFs, OptionsIndefinite life. Data includes symbol, date/time, price, and volume. 1Splits & Dividends: All historical data must be adjusted for stock splits and significant dividends (typically >10%). The standard procedure is to multiply all prior prices (O, H, L, C) by the reciprocal of the split ratio (e.g., 0.5 for a 2-for-1 split) and multiply prior volume by the ratio itself. 1Price Level Distortion: Adjustments cause historical prices in the database to differ from actual transaction prices, which can mislead indicators based on absolute price levels or ranges. Ex-Dividend Gaps: Unadjusted dividend payments create artificial price drops on the ex-dividend date, unfairly penalizing long positions in backtests. 1CommoditiesShort-lived, expiring contracts. Data includes symbol, contract month, date/time, and price. No trade-by-trade volume is reported. 1Continuous Contracts: Individual contracts are too short for system development. The AI must splice contracts, being aware of the method used. Methods include adding/subtracting the price difference or adjusting by the price ratio. A programmatic rollover handler that uses actual front-month data is the preferred, though most complex, solution. 1Price & Ratio Distortion: The add/subtract method distorts percentage-based indicators. The ratio method distorts price-range-based indicators (like ATR). Both methods create a historical series where prices do not represent actual transactions. Perpetual contracts have the same issue. 1Mutual FundsTypically priced once per day at the close by the fund sponsor. Data vendors usually report only this closing price. 1Data is generally simpler, requiring standard quality checks. Ensure that large distributions are handled correctly, similar to stock dividends.Fewer data points per day limit the types of systems that can be developed. Systems must be designed around end-of-day logic.Forex24-hour global market with no central exchange. No traditional daily OHLC bars. Data is often provided by the broker. 1Trading is more akin to intraday. The AI must be able to construct bars from a continuous tick stream. The concept of "close" must be defined by a fixed time (e.g., 5 PM EST).Broker-Dependent Data: The data stream may be from the market-making broker, not a neutral exchange, introducing potential conflicts. Spread: Trading costs are embedded in the bid-ask spread, which must be modeled. 1Directive 2.3: Mandatory Data PurificationAll incoming data, regardless of source or cost, must be considered potentially flawed and must pass through a rigorous purification routine before being admitted to the master database for analysis. This protocol is the AI's immune system, preventing corrupted data from poisoning the entire development and validation process.1The purification routine must automatically scan for and flag:Missing or extra data bars.Inconsistent data points (e.g., Open < Low, High < Close).Anomalous price changes that may indicate unadjusted splits or bad ticks.Upon flagging an error, the AI will follow a predefined correction hierarchy:Attempt to reload the data from the primary source.If the error persists, attempt to reload from a secondary, high-quality source.If external sources cannot resolve the issue, apply a reasonable adjustment (e.g., correcting an obvious decimal point error).As a last resort, delete the entire erroneous bar.Outliers, which are valid but extreme data points, should generally not be removed, as they represent rare but significant market events that a robust system must be ableto withstand.1Directive 2.4: Prohibition of Repainting IndicatorsThe AI's architecture must enforce a strict, inviolable rule: no indicator or signal calculation may depend on data that is received after the time of the calculation. All indicators must be immutable once calculated for a given bar. This is a critical directive against "future leaks," which are a common and catastrophic error in backtester design.1For example, an indicator calculated for the close of trading on Day T may only use data from Day T or earlier. It cannot be influenced by the open of Day T+1. The ZigZag function is a prime example of a repainting indicator that looks into the future to find peaks and troughs, making it useful for research but completely invalid for a tradable system.1 This principle ensures that backtest results are realistic and reflect what could have been achieved in live trading.Section 3: The System Design & Development FrameworkThis section codifies the mandatory, sequential process for creating any new trading system. The AI must follow this staged approach without deviation to ensure that all development is strategically aligned and methodologically sound. This top-down framework prevents the common error of focusing on entry signals before defining performance objectives and risk constraints.Directive 3.1: Adherence to the Three-Stage Design ProcessAll system development must proceed through the following three stages in sequence. A stage cannot begin until the prior stage is complete and its parameters are locked.1Stage 1: Define System Personality and Objectives.Stage 2: Define Financial and Risk Parameters.Stage 3: Develop, Optimize, and Validate System Logic.This structured process is the architectural implementation of the author's philosophy on trader psychology. For a human trader, biases and beliefs can conflict with a trading system's signals. The solution is to design a system that is in agreement with those biases from the start.1 For the AI, "psychology" translates to its "strategic objectives." This staged process ensures that every system developed is inherently aligned with the overarching goals (e.g., high risk-adjusted return, low drawdown) defined in the first two stages, preventing the creation of systems that are technically profitable but strategically misaligned.Directive 3.2: Stage 1 - Define System Personality and ObjectivesBefore any entry or exit logic is considered, the AI must define the system's core purpose and operational style. This stage produces the high-level design specification.1The Objective Function: The AI must select or define the single, multi-component objective function that will be used to measure the "goodness" of the system during optimization. This is the single most important component of the system, as it codifies the desired trading personality (see Section 4).Trading Frequency and Order Style: The AI must define the intended time frame (e.g., daily bars for swing trading, weekly bars for position trading) and the primary order execution method (e.g., Market-on-Open, Market-on-Close). This decision dictates the type of data required and the operational constraints on the system's logic.Directive 3.3: Stage 2 - Define Financial and Risk ParametersThe AI must establish the specific financial context and risk boundaries within which the system will operate. This stage creates a detailed operational "template".1Trading Account: The initial equity allocated to the system.Positions and Leverage: The permissible positions (long-only, short-only, or both) and the maximum allowable leverage or margin. The AI should be directed to test systems that take short positions, as even if they are not traded live, a profitable short system can act as an effective filter to exit long trades in a bear market.1Issue Selection: The category of tradable instruments (e.g., common stocks, ETFs, commodities).Risk Assessment: The maximum percentage of the trading account that can be risked on any single position.Expected Annual Return: A realistic estimate of the target annual return, which serves as a benchmark for validation and Monte Carlo analysis.Directive 3.4: Stage 3 - Develop, Optimize, and ValidateOnly after the template from Stages 1 and 2 is finalized can the AI proceed with the core development loop.1 This stage involves:Developing Entry and Exit Signals: Proposing and coding specific entry and exit logic. Good exits are noted as being able to salvage almost any system.Data Splitting: Dividing the historical data into in-sample (for optimization) and out-of-sample (for validation) periods.Optimization: Running an organized, automated search for the best parameter values for the system's logic, using the in-sample data and judging performance by the objective function score.Validation: Performing a single, final test of the optimized system on the out-of-sample data to determine its likely profitability in live trading.Section 4: Performance Measurement and the Objective FunctionThis section codifies the heart of the evaluation process. The objective function is the ultimate, quantitative arbiter of a system's worth.Directive 4.1: Primacy of the Objective FunctionEvery optimization run and every system evaluation will be judged by a single, pre-defined, multi-component objective function. The system that produces the highest score for this function is, by definition, the "best" system. If the objective function is properly designed to reflect the AI's strategic goals, no other metric matters for the purpose of selection.1 This process is the AI's equivalent of a "utility function" in economic theory, quantifying the desirability of a given performance outcome and allowing for rational, automated decision-making.Directive 4.2: The Objective Function as a Management DecisionThe objective function itself is not a candidate for optimization. It is a strategic management decision set by the AI's human supervisors. The AI must be programmed to flag for review any instance where it consistently identifies a system ranked lower by the objective function as being preferable based on other criteria. Such an event indicates that the objective function is flawed and no longer accurately reflects the desired strategic preferences, requiring modification.1Directive 4.3: Mandatory Calculation of All Performance MetricsFor every backtest, the AI must calculate and report a comprehensive suite of performance metrics. This provides a multi-dimensional view of performance, which is then distilled into the single objective function score. This ensures that all aspects of a system's behavior—profitability, risk, trade characteristics, and equity curve smoothness—are available for analysis.1Metric CategoryMetric NameFormula / DefinitionAuthor's Interpretation / UseProfitabilityNet Profit %Total percentage gain on initial equity.Basic measure of overall profitability.Annual Return % (CAR)The compounded annual rate of return.A normalized measure of annual growth.Profit FactorGross Profit / Gross Loss.Measures how many dollars are won for every dollar lost. A key measure of robustness.Risk-Adjusted ReturnRisk Adjusted Return % (RAR)CAR / Exposure %.Normalizes return for the time the system is in the market.CAR/MaxDDCompounded Annual Return / Maximum System % Drawdown.A crucial measure of return per unit of risk (drawdown). Highly favored metric.Sharpe Ratio(Annualized Return - Risk-Free Rate) / Annualized Std. Dev. of Returns.Measures excess return per unit of volatility.K-RatioLinear regression slope of equity / (Std. Error of equity * sqrt(bars)).Measures the consistency and smoothness of the equity curve growth. Higher is better.Drawdown & RiskMaximum System % DrawdownLargest peak-to-valley percentage decline in portfolio equity.The most important measure of risk for many traders. Minimizing this is critical.Ulcer IndexMeasures the depth and duration of drawdowns.A more comprehensive measure of the "pain" of drawdowns than MaxDD alone.Trade CharacteristicsExpectancy(% Winners * Avg. % Win) - (% Losers * Avg. % Loss).The average profit or loss per trade. Must be positive.Average Bars HeldThe average duration of a trade.Defines the system's time horizon (e.g., swing vs. position trading).Percent WinnersNumber of winning trades / Total number of trades.A psychological metric; high values are often preferred but not necessary for profitability.Payoff RatioAverage Win / Average Loss.The size of the average win relative to the average loss.PRRPessimistic Return Ratio: $\frac{(\#Wins-\sqrt{\#Wins})^{*}AvgWin}{(\#Losses+\sqrt{\#}Losses})^{*}AvgLoss}$A more conservative version of the Profit Factor that penalizes systems with few trades. 1Directive 4.4: Positive Expectancy as a Non-Negotiable FilterA system must demonstrate a positive expectancy to be considered for any further development or validation. Expectancy is the statistical average profit or loss per trade over a large number of trades. It is a mathematical certainty that a system with a negative expectancy will lose money over time, regardless of the money management or position sizing strategy applied. This serves as a fundamental go/no-go filter at the earliest stages of testing.1Section 5: Development Environment & Tooling PhilosophyThis section derives the principles for the AI's internal development and backtesting environment from the author's choice and description of the AmiBroker platform. The architecture of the AI's tools is not independent of the research philosophy; the tools must be designed to serve the methodology.Directive 5.1: Prioritize Execution SpeedThe backtesting engine must be optimized for extreme speed. The quantitative methodology requires massive-scale, iterative testing, including exhaustive parameter optimizations, walk-forward analysis, and Monte Carlo simulations, which can involve millions of individual backtest runs. A slow engine renders this rigorous methodology computationally impractical.1Directive 5.2: Ensure Flexibility and ExtensibilityThe AI's internal programming language and architecture must be flexible, transparent, and extensible. It must support the creation of custom indicators, complex logical structures (e.g., loops and state machines for custom stops), and custom performance metrics. The AI cannot be a "black box" with a fixed set of functions; it must be an open platform that allows for the implementation of novel ideas and sophisticated logic, such as the custom exit strategies detailed later in this roadmap.1Directive 5.3: Mandate Robust Portfolio-Level CapabilitiesThe backtesting engine must be capable of portfolio-level testing, optimization, and management. Real-world trading performance is determined at the portfolio level, not by a single system in isolation. The AI must be able to model and test how multiple systems perform in concert, accounting for shared capital, margin constraints, and the effects of correlated returns across systems. It must also be able to model and test rotational strategies, where capital is dynamically allocated among a basket of instruments based on system signals.1Section 6: Instrument Selection & Pre-analysisThis section codifies the process for creating a filtered, viable universe of tradable instruments before system development begins. This is a crucial step in resource optimization and risk management, ensuring the AI does not waste computational cycles attempting to develop strategies for unsuitable markets. This process is about selecting a "winnable game."Directive 6.1: Apply Mandatory Instrument Selection FiltersThe AI will first execute an exploration run across the entire universe of potential instruments to create a filtered watchlist. Any instrument that fails to meet the following minimum criteria will be excluded from consideration for new system development 1:Sufficient Data History: The instrument must have a minimum period of clean, continuous historical data (e.g., 10 years of daily bars).Minimum Price Level: The instrument's price must have remained above a minimum threshold throughout its history (e.g., lowest close > $2.00). This filter helps avoid the erratic behavior and high frictional costs associated with penny stocks. The AI must be aware that this filter can be distorted by split-adjusted data and should use unadjusted data or account for adjustments when applying it.Sufficient Liquidity: The instrument must demonstrate sufficient liquidity to support trading without significant market impact. A typical threshold is an average daily dollar volume (Close * Volume) over the past year of at least $1,000,000.Directive 6.2: Utilize the ZigZag Function for Research OnlyThe AI will use the ZigZag function as a pre-development research tool to characterize an instrument's volatility and cyclical nature. It is to be used exclusively for analysis and is strictly prohibited from being used as an indicator within any tradable system.1The ZigZag function is a "future-leaking" or "repainting" indicator; it identifies peaks and troughs only after they have formed. Its use in a backtest would produce unrealistically perfect results. However, for research, it is invaluable. By setting the ZigZag percentage parameter to a given drawdown tolerance (e.g., 10%), the AI can run a "perfect" backtest to answer key questions about an instrument's innate character 1:What is the typical profit potential between 10% drawdowns?What is the average holding period (cycle length) for a trend that does not exceed a 10% drawdown?Is the instrument's behavior symmetrical between up-trends and down-trends?This analysis allows the AI to match instruments to system types. For example, an instrument that the ZigZag analysis shows to have long, high-profit cycles would be a good candidate for a trend-following system. Conversely, an instrument with short, regular cycles would be a better candidate for a mean-reversion system. This pre-analysis helps select the right game board for the right game.